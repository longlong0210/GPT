import gradio as gr
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import docx
import re

# 1. HÃ m Ä‘á»c file txt hoáº·c docx tá»« Ä‘Æ°á»ng dáº«n
def read_file(file_path):
    text = ""
    if file_path.endswith(".txt"):
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
    elif file_path.endswith(".docx"):
        doc = docx.Document(file_path)
        text = "\n".join([para.text.strip() for para in doc.paragraphs if para.text.strip()])
    return text

# 2. HÃ m tÃ¡ch cÃ¢u vÃ  chia thÃ nh chunk 10 cÃ¢u
def split_into_chunks(text, sentences_per_chunk=10):
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-ZÃ€ÃÃ‚ÃƒÃˆÃ‰ÃŠÃŒÃÃ’Ã“Ã”Ã•Ã™ÃšÄ‚ÄÄ¨Å¨Æ Æ¯áº -á»¹])', text.strip())
    sentences = [s.strip() for s in sentences if s.strip()]
    chunks = []
    for i in range(0, len(sentences), sentences_per_chunk):
        chunk = " ".join(sentences[i:i + sentences_per_chunk])
        chunks.append(chunk)
    return chunks

# 3. Táº£i model tiáº¿ng Viá»‡t
model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')

# 4. HÃ m tÃ¬m kiáº¿m chunk liÃªn quan nháº¥t
def search_best_sentences(data_chunks, query, top_n=3):
    # TÃ¬m chunk liÃªn quan nháº¥t
    chunk_embeddings = model.encode(data_chunks)
    query_embedding = model.encode([query])[0]
    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]
    top_chunk_index = similarities.argmax()
    best_chunk = data_chunks[top_chunk_index]

    # TÃ¡ch cÃ¢u trong chunk Ä‘Ã³
    sentences = re.split(r'(?<=[.!?])\s+', best_chunk.strip())
    sentences = [s.strip() for s in sentences if s.strip()]

    # TÃ­nh similarity tá»«ng cÃ¢u
    sentence_embeddings = model.encode(sentences)
    sentence_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]

    # Láº¥y top_n cÃ¢u tá»‘t nháº¥t
    top_indices = sentence_scores.argsort()[-top_n:][::-1]
    best_sentences = [f"{sentences[i]} (score: {sentence_scores[i]:.4f})" for i in top_indices]

    return "\n".join(best_sentences)

# 5. HÃ m xá»­ lÃ½ toÃ n bá»™
def process_data(file_path, text_input, query):
    if file_path is not None:
        text = read_file(file_path)
    else:
        text = text_input
    
    if not text.strip():
        return "Kho dá»¯ liá»‡u trá»‘ng!"

    chunks = split_into_chunks(text, sentences_per_chunk=10)
    result = search_best_sentences(chunks, query, top_n=3)
    
    return f"ğŸ” CÃ¡c cÃ¢u liÃªn quan nháº¥t:\n{result}"

# 6. Giao diá»‡n Gradio
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ” TÃ¬m kiáº¿m cÃ¢u liÃªn quan nháº¥t (txt & docx)")
    
    with gr.Row():
        file_input = gr.File(label="Upload file (.txt hoáº·c .docx)", type="filepath")
        text_input = gr.Textbox(label="Hoáº·c nháº­p trá»±c tiáº¿p kho dá»¯ liá»‡u", lines=6)
    
    query_input = gr.Textbox(label="Nháº­p cÃ¢u há»i", lines=2)
    search_btn = gr.Button("TÃ¬m kiáº¿m")
    output_box = gr.Textbox(label="Káº¿t quáº£", lines=10)
    
    search_btn.click(fn=process_data, 
                     inputs=[file_input, text_input, query_input],
                     outputs=output_box)

if __name__ == "__main__":
    demo.launch()
