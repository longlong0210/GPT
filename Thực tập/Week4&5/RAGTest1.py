import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import os
import textwrap

# --- PHáº¦N 1: Cáº¤U HÃŒNH GOOGLE AI ---
try:
    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
    if not GOOGLE_API_KEY:
        GOOGLE_API_KEY = "AIzaSyABLonsDEQ7veJFWZf6lLlHvtPw9K4lBMs"  # Thay báº±ng API key cá»§a báº¡n
        print("âš ï¸ Cáº£nh bÃ¡o: API Key Ä‘ang Ä‘Æ°á»£c Ä‘áº·t trá»±c tiáº¿p trong code.")
    genai.configure(api_key=GOOGLE_API_KEY)
    print("âœ… ÄÃ£ cáº¥u hÃ¬nh Google AI thÃ nh cÃ´ng.")
except Exception as e:
    print(f"âŒ Lá»—i cáº¥u hÃ¬nh Google AI: {e}")


# --- PHáº¦N 2: Há»† THá»NG RAG ---
class RAGSystem:
    def __init__(self, corpus, embedding_model_name='bkai-foundation-models/vietnamese-bi-encoder'):
        print("ğŸš€ Äang khá»Ÿi táº¡o há»‡ thá»‘ng RAG...")
        self.embedding_model = SentenceTransformer(embedding_model_name)
        self.generation_model = genai.GenerativeModel('gemini-2.5-flash')
        self.vector_db, self.chunks = self._build_vector_db(corpus)
        print("âœ… Há»‡ thá»‘ng RAG Ä‘Ã£ sáºµn sÃ ng.")

    def _build_vector_db(self, corpus):
        # Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n ~200 kÃ½ tá»±
        text_splitter = textwrap.TextWrapper(width=200, break_long_words=False, replace_whitespace=False)
        chunks = text_splitter.wrap(corpus)
        print(f"ğŸ“„ TÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chia thÃ nh {len(chunks)} Ä‘oáº¡n.")

        # Táº¡o embedding
        print("ğŸ”„ Äang táº¡o vector embedding...")
        chunk_embeddings = self.embedding_model.encode(chunks, convert_to_tensor=True, show_progress_bar=True)
        chunk_embeddings = chunk_embeddings.cpu().numpy()

        # FAISS Index
        d = chunk_embeddings.shape[1]
        index = faiss.IndexFlatL2(d)
        index.add(chunk_embeddings)
        print("ğŸ“¦ ÄÃ£ xÃ¢y dá»±ng xong Vector DB.")
        return index, chunks

    def retrieve(self, query, k=3):
        query_embedding = self.embedding_model.encode([query])
        _, indices = self.vector_db.search(query_embedding, k)
        return [self.chunks[i] for i in indices[0]]

    def generate(self, relevant_chunks, query):
        context = "\n\n".join(relevant_chunks)
        prompt = f"""
        Dá»±a vÃ o ngá»¯ cáº£nh dÆ°á»›i Ä‘Ã¢y, hÃ£y tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch tá»± nhiÃªn, chÃ­nh xÃ¡c.
        Náº¿u thÃ´ng tin khÃ´ng cÃ³ trong ngá»¯ cáº£nh, hÃ£y nÃ³i rÃµ Ä‘iá»u Ä‘Ã³.

        **Ngá»¯ cáº£nh:**
        {context}

        **CÃ¢u há»i:**
        {query}

        **CÃ¢u tráº£ lá»i:**
        """
        response = self.generation_model.generate_content(prompt)
        return response.text

    def ask(self, query):
        print("\nğŸ” Äang truy xuáº¥t thÃ´ng tin...")
        retrieved_chunks = self.retrieve(query)
        print("âœï¸ Äang táº¡o cÃ¢u tráº£ lá»i...")
        return self.generate(retrieved_chunks, query)


# --- PHáº¦N 3: CHáº Y CHÆ¯Æ NG TRÃŒNH ---
if __name__ == "__main__":
    # Nháº­p dá»¯ liá»‡u vÃ  cÃ¢u há»i
    corpus_input = input("ğŸ“¥ Nháº­p dá»¯ liá»‡u vÄƒn báº£n cá»§a báº¡n:\n")
    question_input = input("\nâ“ Nháº­p cÃ¢u há»i cá»§a báº¡n:\n")

    # Khá»Ÿi táº¡o há»‡ thá»‘ng
    rag_system = RAGSystem(corpus_input)

    # Tráº£ lá»i cÃ¢u há»i
    answer = rag_system.ask(question_input)
    print("\nğŸ¤– CÃ¢u tráº£ lá»i tá»« RAG:")
    print(answer)
