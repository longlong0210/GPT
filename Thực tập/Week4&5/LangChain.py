import os
import re
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.chains.retrieval_qa.base import RetrievalQA

# ====== 1. C·∫•u h√¨nh API key ======
os.environ["GOOGLE_API_KEY"] = "AIzaSyABLonsDEQ7veJFWZf6lLlHvtPw9K4lBMs"  # <-- thay b·∫±ng key th·∫≠t

# ====== 2. D·ªØ li·ªáu m·∫´u ======
raw_text = """
T·∫•n c√¥ng brute-force l√† m·ªôt ph∆∞∆°ng ph√°p b·∫ª kh√≥a ph·ªï bi·∫øn . M·ªôt cu·ªôc t·∫•n c√¥ng brute-force li√™n quan ƒë·∫øn vi·ªác 'ƒëo√°n' t√™n ng∆∞·ªùi d√πng v√† m·∫≠t kh·∫©u ƒë·ªÉ truy c·∫≠p tr√°i ph√©p v√†o h·ªá th·ªëng, hacker s·∫Ω s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p th·ª≠ v√† sai ƒë·ªÉ c·ªë g·∫Øng ƒëo√°n th√¥ng tin ƒëƒÉng nh·∫≠p h·ª£p l·ªá. Ngo√†i ra ta c√≥ th·ªÉ s·ª≠ d·ª•ng brute-force ƒë·ªÉ khai th√°c OTP, Timestamp , Cookie, vv... Tuy nhi√™n b√†i vi·∫øt n√†y s·∫Ω t·∫≠p trung v√†o brute-force m·∫≠t kh·∫©u ƒë·ªÉ ƒëƒÉng nh·∫≠p t√†i kho·∫£n ng∆∞·ªùi d√πng.

C√°c cu·ªôc t·∫•n c√¥ng n√†y th∆∞·ªùng ƒë∆∞·ª£c t·ª± ƒë·ªông h√≥a b·∫±ng c√°ch s·ª≠ d·ª•ng danh s√°ch c√°c t·ª´ g·ªìm t√™n ng∆∞·ªùi d√πng v√† m·∫≠t kh·∫©u th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ t·ªët nh·∫•t. Vi·ªác s·ª≠ d·ª•ng c√°c c√¥ng c·ª• chuy√™n d·ª•ng c√≥ kh·∫£ nƒÉng cho ph√©p hacker th·ª±c hi·ªán ƒëƒÉng nh·∫≠p 1 c√°ch t·ª± ƒë·ªông nhi·ªÅu l·∫ßn v·ªõi t·ªëc ƒë·ªô cao.

Brute-force l√† m·ªôt ph∆∞∆°ng ph√°p t·∫•n c√¥ng ƒë∆°n gi·∫£n v√† c√≥ t·ª∑ l·ªá th√†nh c√¥ng cao. B·ªüi v√¨ t√πy thu·ªôc v√†o ƒë·ªô d√†i v√† ƒë·ªô ph·ª©c t·∫°p c·ªßa m·∫≠t kh·∫©u, vi·ªác b·∫ª kh√≥a m·∫≠t kh·∫©u c√≥ th·ªÉ m·∫•t t·ª´ v√†i gi√¢y ƒë·∫øn nhi·ªÅu nƒÉm. Do ƒë√≥ c√°c trang web s·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c ƒëƒÉng nh·∫≠p d·ª±a tr√™n m·∫≠t kh·∫©u ho√†n to√†n c√≥ th·ªÉ r·∫•t d·ªÖ b·ªã t·∫•n c√¥ng n·∫øu h·ªç kh√¥ng th·ª±c hi·ªán ƒë·∫ßy ƒë·ªß bi·ªán ph√°p b·∫£o v·ªá b·∫°o l·ª±c.
""".strip()

# ====== 3. T√°ch vƒÉn b·∫£n theo c√¢u & chunk ‚Äúƒë·∫πp‚Äù ======
# ∆Øu ti√™n ng·∫Øt theo ƒëo·∫°n, xu·ªëng d√≤ng, d·∫•u c√¢u ƒë·ªÉ tr√°nh ch·∫ª ngang c√¢u
splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ". ", "? ", "! ", "; "],
    keep_separator=False,
    chunk_size=500,       # tƒÉng size ƒë·ªÉ gi·ªØ nguy√™n c√¢u/ƒëo·∫°n
    chunk_overlap=80      # ch·ªìng l·∫•p nh·∫π ƒë·ªÉ kh√¥ng m·∫•t ng·ªØ c·∫£nh
)

base_doc = Document(page_content=raw_text, metadata={"source": "kb"})
chunks = splitter.split_documents([base_doc])

# G·∫Øn th√™m metadata nh·∫≠n di·ªán chunk
for i, d in enumerate(chunks):
    d.metadata.update({"chunk_id": i})

# ====== 4. Embedding + FAISS ======
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
vector_db = FAISS.from_documents(chunks, embeddings)

# ====== 5. LLM (Gemini) ======
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)

# ====== 6. Retriever (MMR ƒë·ªÉ gi·∫£m tr√πng l·∫∑p) ======
retriever = vector_db.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 3,          # s·ªë chunk cu·ªëi c√πng tr·∫£ v·ªÅ
        "fetch_k": 15,   # s·ªë l∆∞·ª£ng l·∫•y r·ªông ban ƒë·∫ßu tr∆∞·ªõc khi MMR ch·ªçn l·ªçc
        "lambda_mult": 0.5
    }
)

# ====== 7. Chain RAG ======
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    return_source_documents=True
)

# ====== 8. Truy v·∫•n th·ª≠ ======
query = "Brute force l√† g√¨ v√† t·∫•n c√¥ng nh∆∞ th·∫ø n√†o?"
result = qa_chain.invoke({"query": query})

print("üí¨ C√¢u h·ªèi:", query)
print("ü§ñ Tr·∫£ l·ªùi:", result["result"])

print("\nüìö Ngu·ªìn d·ªØ li·ªáu (chunk ƒë·∫ßy ƒë·ªß):")
for doc in result["source_documents"]:
    cid = doc.metadata.get("chunk_id", "?")
    print(f"\n--- Chunk #{cid} ---")
    print(doc.page_content.strip())
