import os
import re
import unicodedata
import google.generativeai as genai
import fitz  # PyMuPDF
import docx
import openpyxl
from pptx import Presentation
import pytesseract
from PIL import Image
import io

# =========================================================
# PH·∫¶N 1: C·∫§U H√åNH V√Ä C√ÅC H·∫∞NG S·ªê
# =========================================================

# --- C·∫•u h√¨nh Google AI ---
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    # VUI L√íNG THAY TH·∫æ B·∫∞NG API KEY C·ª¶A B·∫†N
    GOOGLE_API_KEY = "AIzaSyABLonsDEQ7veJFWZf6lLlHvtPw9K4lBMs"

try:
    genai.configure(api_key=GOOGLE_API_KEY)
    print("‚úÖ ƒê√£ c·∫•u h√¨nh Google AI th√†nh c√¥ng.")
except Exception as e:
    print(f"‚ùå L·ªói c·∫•u h√¨nh Google AI: {e}")

# --- Danh s√°ch t·ª´ d·ª´ng Ti·∫øng Vi·ªát ---
VIETNAMESE_STOP_WORDS = [
    "v√†", "l√†", "m√†", "th√¨", "c·ªßa", "·ªü", "t·∫°i", "b·ªã", "b·ªüi", "c·∫£", "c√°c", "c√°i", "c·∫ßn",
    "cho", "ch·ª©", "ch∆∞a", "c√≥", "c≈©ng", "ƒë√£", "ƒëang", "ƒë√¢y", "ƒë·ªÉ", "ƒë·∫øn", "ƒë·ªÅu", "ƒëi·ªÅu",
    "do", "ƒë√≥", "ƒë∆∞·ª£c", "khi", "kh√¥ng", "l·∫°i", "l√™n", "l√∫c", "m·ªói", "m·ªôt", "n√™n", "n·∫øu",
    "ngay", "nh∆∞", "nh∆∞ng", "nh·ªØng", "n∆°i", "n·ªØa", "ph·∫£i", "qua", "ra", "r·∫±ng", "r·∫•t",
    "r·ªìi", "sau", "s·∫Ω", "so", "s·ª±", "t·∫°i", "theo", "th√¨", "tr√™n", "tr∆∞·ªõc", "t·ª´", "v·∫´n",
    "v√†o", "v·∫≠y", "v·ªÅ", "v√¨", "vi·ªác", "v·ªõi", "v·ª´a"
]

# =========================================================
# PH·∫¶N 2: C√ÅC H√ÄM X·ª¨ L√ù D·ªÆ LI·ªÜU
# =========================================================

# --- C√°c h√†m tr√≠ch xu·∫•t n·ªôi dung t·ª´ file ---

def extract_text_from_pdf(file_path):
    """Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ PDF, t·ª± ƒë·ªông d√πng OCR n·∫øu c·∫ßn."""
    text = ""
    try:
        with fitz.open(file_path) as doc:
            for page in doc:
                text += page.get_text()
        
        if len(text.strip()) < 100:
            print(f"   - VƒÉn b·∫£n trong file '{os.path.basename(file_path)}' √≠t. Th·ª≠ d√πng OCR...")
            ocr_text = ""
            with fitz.open(file_path) as doc:
                for page_num, page in enumerate(doc):
                    pix = page.get_pixmap(dpi=300)
                    img_bytes = pix.tobytes("png")
                    image = Image.open(io.BytesIO(img_bytes))
                    page_text = pytesseract.image_to_string(image, lang='vie+eng')
                    ocr_text += page_text + "\n"
            return ocr_text
    except Exception as e:
        return f"L·ªói khi x·ª≠ l√Ω PDF '{file_path}': {e}"
    return text

def extract_text_from_docx(file_path):
    doc = docx.Document(file_path)
    return "\n".join([p.text for p in doc.paragraphs])

def extract_text_from_pptx(file_path):
    prs = Presentation(file_path)
    return "\n".join([shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text")])

def extract_text_from_xlsx(file_path):
    wb = openpyxl.load_workbook(file_path)
    texts = []
    for sheet in wb.worksheets:
        for row in sheet.iter_rows():
            row_data = [str(cell.value) for cell in row if cell.value]
            if row_data:
                texts.append(" ".join(row_data))
    return "\n".join(texts)

def extract_text_from_txt(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()

# --- H√†m t·ªïng h·ª£p: ƒê·ªçc file ---

def read_file_content(file_path):
    """T·ª± ph√°t hi·ªán lo·∫°i file v√† ƒë·ªçc n·ªôi dung."""
    extractors = {
        ".pdf": extract_text_from_pdf, ".docx": extract_text_from_docx,
        ".pptx": extract_text_from_pptx, ".xlsx": extract_text_from_xlsx,
        ".txt": extract_text_from_txt
    }
    ext = os.path.splitext(file_path)[1].lower()
    if ext in extractors:
        try:
            return extractors[ext](file_path)
        except Exception as e:
            return f"L·ªói khi ƒë·ªçc file {os.path.basename(file_path)}: {e}"
    return f"L·ªói: ƒê·ªãnh d·∫°ng file '{ext}' ch∆∞a ƒë∆∞·ª£c h·ªó tr·ª£."

# --- C√°c h√†m l√†m s·∫°ch v√† ph√¢n ƒëo·∫°n ---

def normalize_and_clean_text(text: str) -> str:
    """L√†m s·∫°ch v√† chu·∫©n h√≥a m·ªôt ƒëo·∫°n vƒÉn b·∫£n."""
    text = unicodedata.normalize('NFC', text)
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    words = text.split()
    filtered_words = [word for word in words if word not in VIETNAMESE_STOP_WORDS]
    text = " ".join(filtered_words)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def chunk_text_by_sentence(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> list[str]:
    """Ph√¢n ƒëo·∫°n vƒÉn b·∫£n m·ªôt c√°ch th√¥ng minh, gi·ªØ tr·ªçn v·∫πn c√¢u."""
    sentences = re.split(r'(?<=[.?!])\s+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    if not sentences: return []

    chunks = []
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= chunk_size:
            current_chunk += sentence + " "
        else:
            if current_chunk: chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
    if current_chunk: chunks.append(current_chunk.strip())
    return chunks

# =========================================================
# PH·∫¶N 3: H√ÄM T·ªîNG H·ª¢P G·ªåI AI
# =========================================================
def process_and_analyze(file_paths, user_prompt):
    """
    Th·ª±c hi·ªán to√†n b·ªô quy tr√¨nh: ƒê·ªçc -> L√†m s·∫°ch -> Ph√¢n ƒëo·∫°n -> G·ªçi AI.
    """
    all_chunks = []
    print("\n‚è≥ B·∫Øt ƒë·∫ßu quy tr√¨nh x·ª≠ l√Ω file...")
    for file_path in file_paths:
        # 1. ƒê·ªçc file
        print(f"   - [1/3] ƒêang ƒë·ªçc file: {os.path.basename(file_path)}")
        raw_content = read_file_content(file_path)
        if raw_content.startswith("L·ªói:"):
            print(f"     -> {raw_content}")
            continue
        
        # 2. L√†m s·∫°ch
        print("   - [2/3] ƒêang l√†m s·∫°ch v√† chu·∫©n h√≥a n·ªôi dung...")
        cleaned_content = normalize_and_clean_text(raw_content)
        
        # 3. Ph√¢n ƒëo·∫°n
        print("   - [3/3] ƒêang ph√¢n ƒëo·∫°n vƒÉn b·∫£n...")
        chunks = chunk_text_by_sentence(cleaned_content)
        all_chunks.extend(chunks)
        print(f"     -> Ho√†n t·∫•t, t·∫°o ra {len(chunks)} ƒëo·∫°n.")

    if not all_chunks:
        return "Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ ph√¢n t√≠ch sau khi x·ª≠ l√Ω."

    combined_text = "\n\n---\n\n".join(all_chunks)
    
    print("\nüß† ƒêang g·ª≠i d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ƒë·∫øn Google AI...")
    model = genai.GenerativeModel("gemini-2.5-flash")
    prompt = f"D·ª±a tr√™n c√°c ƒëo·∫°n vƒÉn b·∫£n d∆∞·ªõi ƒë√¢y, h√£y th·ª±c hi·ªán y√™u c·∫ßu sau: '{user_prompt}'.\n\nD·ªØ li·ªáu:\n{combined_text}"

    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"L·ªói khi g·ªçi Google AI: {e}"

# =========================================================
# PH·∫¶N 4: CH·∫†Y CH∆Ø∆†NG TR√åNH CH√çNH
# =========================================================
if __name__ == "__main__":
    file_paths = []
    print("\nNh·∫≠p ƒë∆∞·ªùng d·∫´n ƒë·∫øn c√°c file b·∫°n mu·ªën ph√¢n t√≠ch.")
    print("G√µ 'xong' ho·∫∑c 'done' tr√™n m·ªôt d√≤ng ri√™ng ƒë·ªÉ k·∫øt th√∫c.")
    
    while True:
        path = input(f"ƒê∆∞·ªùng d·∫´n file {len(file_paths) + 1}: ").strip()
        if path.lower() in ['xong', 'done']: break
        if os.path.exists(path):
            file_paths.append(path)
            print(f"-> ƒê√£ th√™m file: {os.path.basename(path)}")
        else:
            print("-> L·ªói: File kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")

    if file_paths:
        user_prompt = input("\nNh·∫≠p y√™u c·∫ßu c·ªßa b·∫°n v·ªÅ c√°c t√†i li·ªáu tr√™n (v√≠ d·ª•: t√≥m t·∫Øt n·ªôi dung):\n")
        
        result = process_and_analyze(file_paths, user_prompt)
        
        print("\n" + "="*50)
        print("‚úÖ K·∫æT QU·∫¢ PH√ÇN T√çCH T·ª™ GOOGLE AI:")
        print("="*50)
        print(result)
    else:
        print("\nB·∫°n ch∆∞a ch·ªçn file n√†o. Ch∆∞∆°ng tr√¨nh k·∫øt th√∫c.")